{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building_digit_recog_ann_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbFoB5Rj4reU9a+38DlNRt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sequeirayeslin/digit_recog_ann_from_scratch/blob/main/Building_digit_recog_ann_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8q-uDiJ1kUq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import e"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unipolar_sigmoid = lambda x: 1/(1+e**(-x))\n",
        "unipolar_sigmoid_diff = lambda x: x*(1-x)\n",
        "\n",
        "\n",
        "act_dict={\n",
        "    \"unipolar_sigmoid\": [unipolar_sigmoid, unipolar_sigmoid_diff]\n",
        "    #, \"relu\": [relu, relu_diff]\n",
        "}"
      ],
      "metadata": {
        "id": "Pc141WZ_Usw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_w_limit = 1\n",
        "default_act = \"unipolar_sigmoid\"\n",
        "biasInput = 1\n",
        "\n",
        "class Neuron:\n",
        "  def __init__(self,inputs, w_limit=default_w_limit, act=default_act):\n",
        "    \n",
        "    self.W = np.random.rand(inputs+1)*w_limit *2 - w_limit\n",
        "    #self.W = np.random.rand(inputs)*w_limit *2 - w_limit\n",
        "\n",
        "    self.act, self.act_diff = act_dict[act]\n",
        "  \n",
        "  def net(self, X):\n",
        "\n",
        "    X = np.append(X, biasInput)\n",
        "\n",
        "    return np.dot(self.W, X)\n",
        "  \n",
        "  def out(self, X):\n",
        "\n",
        "    net = self.net(X)\n",
        "    return self.act(net)\n",
        "\n",
        "class Layer:\n",
        "\n",
        "  def __init__(self, neurons, inputs, w_limit=default_w_limit, act=default_act):\n",
        "\n",
        "    self.neurons = [Neuron(inputs,w_limit,act) for _ in range(neurons)]\n",
        "    self.act = act_dict[act][0]\n",
        "    self.act_diff = act_dict[act][1]\n",
        "\n",
        "    self.out_mem = None\n",
        "    self.diffs = None\n",
        "    self.error= None\n",
        "    self.in_mem = None\n",
        "\n",
        "  def out(self, X):\n",
        "\n",
        "    self.in_mem = np.append(X, biasInput)\n",
        "    #self.in_mem = np.array(X)\n",
        "\n",
        "    out = [neuron.out(X) for neuron in self.neurons]\n",
        "    out = np.array(out)\n",
        "\n",
        "    self.out_mem = out\n",
        "    self.diffs = self.act_diff(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "class Network:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.layers=[]\n",
        "\n",
        "  def out(self, X):\n",
        "\n",
        "    in_data = X\n",
        "\n",
        "    for layer in self.layers:\n",
        "      out = layer.out(in_data)\n",
        "      in_data = out\n",
        "    \n",
        "    return out\n",
        "\n",
        "  def addInLayer(self, neurons, inputs, w_limit=default_w_limit, act=default_act):\n",
        "    self.layers.append(Layer(neurons, inputs, w_limit, act))\n",
        "\n",
        "  def addLayer(self, neurons, w_limit=default_w_limit, act=default_act):\n",
        "    inputs = len(self.layers[-1].neurons)\n",
        "    self.layers.append(Layer(neurons, inputs, w_limit, act))\n",
        "\n",
        "  def calc_error(self,Y):\n",
        "    \n",
        "    for layer in reversed(self.layers):\n",
        "\n",
        "      if layer == self.layers[-1]:\n",
        "        layer.error = (Y-layer.out_mem)\n",
        "\n",
        "      else:\n",
        "        layer.error = []\n",
        "        for i in range(len(layer.neurons)):\n",
        "          temp = [neuron.W[i] for neuron in last_layer.neurons]\n",
        "          temp = temp * last_layer.error * last_layer.diffs\n",
        "          temp = np.sum(temp)\n",
        "          layer.error.append(temp)\n",
        "        layer.error = np.array(layer.error)\n",
        "\n",
        "      last_layer = layer\n",
        "\n",
        "\n",
        "  def train(self,X,Y,C):\n",
        "\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    self.out(X)\n",
        "    self.calc_error(Y)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      for i,neuron in enumerate(layer.neurons):\n",
        "        neuron.W = neuron.W + C * layer.diffs[i] * layer.error[i] * layer.in_mem\n",
        "  \n",
        "  def test_classifier(self, X, Y):\n",
        "\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "    \n",
        "    total = len(X)\n",
        "    correct = 0\n",
        "\n",
        "    for x,y in zip(X,Y):\n",
        "\n",
        "      out = self.out(x)\n",
        "      isItCorrect = np.where(y == np.amax(y)) [0] == np.where(out == np.amax(out)) [0]\n",
        "      isItCorrect = int(isItCorrect)\n",
        "      correct += isItCorrect\n",
        "    \n",
        "    print('{}% accuracy'.format(correct/total * 100))"
      ],
      "metadata": {
        "id": "w7QN_cN74uox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing ONLY handwritten number detection training set from tenserflow\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
        "\n",
        "y_train = [[int(y==num) for num in range(10)]for y in y_train]  \n",
        "x_train = [x.flatten()/255 for x in x_train]\n",
        "\n",
        "y_test = [[int(y==num) for num in range(10)]for y in y_test]\n",
        "x_test = [x.flatten()/255 for x in x_test]"
      ],
      "metadata": {
        "id": "Z9LP9DI6amXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brain = Network()\n",
        "brain.addInLayer(neurons = 30, inputs = 784)\n",
        "brain.addLayer(neurons=10)"
      ],
      "metadata": {
        "id": "NeZqsLbMazWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "AqA-afCYC9NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "  clear_output()\n",
        "  i_10 = round(i/epochs * 10)\n",
        "\n",
        "  print('Training (epoch {})... \\n['.format(i+1) + '--'*(i_10) + '->' + '  '*(10-(i_10+1)) + ']', end = '')\n",
        "\n",
        "  for x,y in zip(x_train, y_train):\n",
        "    brain.train(x,y,C=0.1)\n",
        "\n",
        "clear_output()\n",
        "print('Traning complete! \\n[--------------------]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItYuN2zXdf_G",
        "outputId": "d50ee798-f53a-4316-8442-1df97f1feac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traning complete! \n",
            "[--------------------]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brain.test_classifier(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wVAwW8EHMy3N",
        "outputId": "a8b1a949-12dc-4b06-9871-705e7035a110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'95.07% accuracy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brain.test_classifier(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qtQslhG5NlYH",
        "outputId": "41ed7399-3a12-465f-9eca-4a6cb5bba3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'96.42166666666667% accuracy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}
